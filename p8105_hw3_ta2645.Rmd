---
title: "P8105 Homework 3"
author: "Tara Ahi"
date: "10/19/2021"
output: github_document
---

```{r setup, message = FALSE}

library(tidyverse)
library(p8105.datasets)
library(httr)
library(jsonlite)

```

# Problem 1

## Instacart

Most popular aisles and number of items ordered:

```{r, aisle_count, echo = TRUE, message=FALSE}

data("instacart")
n_aisle = 
  instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))

head(n_aisle, 5)
  
```

This tells us that what the most popular aisles are and how many items are ordered from each:

* fresh vegetables with 150,609
* fresh fruits with 150,473
* packaged vegetables and fruits with 78,493
* yogurt with 55,240
* packaged cheese with 41,699

### Describing the data

The **instacart** data set has `r nrow(instacart)` observatoins and `r ncol(instacart)` variables. There are `r nrow(n_aisle)` aisles. 

### Plot for aisles with more than 10,000 items ordered

```{r, aisle plot, echo=FALSE}

aisle_plot = 
  n_aisle %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle,n)) %>% 
    ggplot(
      aes(x = aisle, y = n)) +
      labs(y = "Number of items ordered", x = "Aisle") +
  geom_bar(stat = "identity", fill = "plum") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

aisle_plot

ggsave("aisle_plot.pdf", aisle_plot, width = 10, height = 5)

```

This plot displays the aisles with over 10,000 items ordered along with the number of items ordered from each. The graph is ordered from least to greatest moving left to right. 


### Most popular items from Baking Ingredients, Dog Food Care and Packaged Vegetables and Fruits

```{r, three_aisles, echo = FALSE}

popular_aisles = 
  instacart %>% 
  janitor::clean_names() %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables and fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  rename("frequency" = n) %>% 
  mutate(product_rank = min_rank(desc(frequency)),
         product_name = tolower(product_name)) %>% 
  filter(product_rank < 4) %>% 
  arrange(aisle, product_rank)

knitr::kable(popular_aisles, caption = "Three most popular items from aisles Baking Ingredients, Dog Food Care, and Packaged Vegetables and Fruits")

```

As we can see from the table, there is no information on items from the **packaged fruits vegetables** aisle. The three most popular items from **baking ingredients** are _light brown sugar_, _pure baking soda_ and _organic vanilla extract_. The three most popular items from **dog food care** are _organix grain free chicken & vegetable dog food_, _organix chicken & brown rice recipe_ and _original dry dog_. 

### Information on Pink Lady Apples and Coffee Ice Cream

The below table shows the mean hour of day when Pink Lady apples and coffee ice cream are ordered each day of the week. 

```{r, apples_icecream, echo=FALSE, message=FALSE}

mean_hour = 
  instacart %>% 
  mutate(product_name = tolower(product_name), 
         order_dow = recode(order_dow,
                                                                 "0" = "Monday",
                                                                 "1" = "Tuesday",
                                                                 "2" = "Wednesday",
                                                                 "3" = "Thursday",
                                                                 "4" = "Friday",
                                                                 "5" = "Saturday",
                                                                 "6" = "Sunday")) %>% 
  filter(product_name %in% c("pink lady apple", "coffee ice cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(hour = mean(order_hour_of_day)) %>% 
  pivot_wider(names_from = order_dow, values_from = hour)

       knitr::kable(mean_hour, digits = 0, caption = "Average hour of day when Pink Lady apples and coffee ice cream are ordered each day of the week")                                                   

```


# Problem 2

## BRFSS Data

Here we are cleaning the BRFSS data by formatting it with variable names, narrowing to the "Overall Health" topic, looking at the "Excellent", "Very Good", "Good" and "Poor" responses, and ordering them appropriately. 

```{r, brfss, warning = FALSE}
data("brfss_smart2010")

cleaned_brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health", response %in% c("Excellent", "Very Good", "Good", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Excellent", "Very Good", "Good", "Poor"))) %>% 
  separate(locationdesc, c("state", "location", "rest")) %>% 
  unite(location, c("location", "rest")) %>% 
  arrange(response)

cleaned_brfss
```

### States with seven or more locations in 2002 and 2010


```{r, brfss_2002}
brfss_2002 =
  cleaned_brfss %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  distinct(location) %>% 
  count(state) %>% 
  filter(n > 6)

knitr::kable(brfss_2002)
```

The above table shows the states that had seven or more locations in 2002 (along with how many locations each has).

```{r, brfss_2010}
brfss_2010 =
  cleaned_brfss %>% 
  filter(year == "2010") %>% 
  group_by(state) %>% 
  distinct(location) %>% 
  count(state) %>% 
  filter(n > 6)

knitr::kable(brfss_2010)
```

The above table shows the states that had seven or more locations in 2010 (along with how many locations each has).

#### Spaghetti plot:

The below plot shows the average value over time within a state from 2002 to 2010 (note: `group` was not used upon advice in office hours).

```{r, warning=FALSE}
cleaned_brfss %>% 
  select(year, locationabbr, response, data_value, geo_location) %>% 
  filter(response == "Excellent") %>% 
  group_by(locationabbr, year) %>% 
  mutate(data_avg = mean(data_value)) %>% 
  select(year, locationabbr, data_avg) %>% 
  
  ggplot(aes(x = year, y = data_avg, group = locationabbr, color = locationabbr)) +
  geom_line() +
  theme(legend.position = "right")
```


#### Two panel plot:

The below plot shows the distribution of `data_value` for responses _Poor_ to _Excellent_  among locations in New York for the years 2006 and 2010.

```{r, warning=FALSE}
cleaned_brfss %>% 
  filter(locationabbr == "NY", year %in% c("2006", "2010")) %>% 
  ggplot(aes(x = response, y = data_value, color = response)) +
  geom_boxplot() +
  facet_grid(. ~ year)
  
```



# Problem 3

## Importing and Cleaning Accelerometer Data

Load, tidy and wrangle the data:

```{r, message=FALSE}

accelerometer_df = read_csv("./data/accel_data.csv")

cleaned_accel_df = 
  accelerometer_df %>% 
  pivot_longer(
    cols = activity.1:activity.1440,
    names_to = "activity_number",
    values_to = "activity_counts",
    names_prefix = "activity.") %>% 
  mutate(weekend_day = (day == "Saturday" | day == "Sunday"))

```

I have input useful variable names, created a weekend (vs weekday) variable, and encoded reasonable classes. 
The cleaned data set contains **50,400** observations and **6** variables. The data contains a week number, day ID, day of the week, an activity number, activity count and whether or not the observation is on a weekend or not.

```{r}
total_activity = 
  cleaned_accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity_counts = sum(activity_counts))

knitr::kable(total_activity)
```

The table was not much help here so I created a bar graph to compare values, which is easier to read. There seems to be more consistent activity in the first half of the the time period, and a couple of low points are noted around day 24 and day 32, seen below.

```{r}
ggplot(total_activity, aes(x = day_id, y = total_activity_counts)) +
  geom_point() +
  geom_col(position = "dodge") +
  labs(title = "Total Daily Activity", x = "Day", y = "Activity")
```

We are now looking at activity over the course of the day by day of the week:

```{r, 24hr_activity, message=FALSE, warning=FALSE}

cleaned_accel_df %>% 
  mutate(activity_number = as.numeric(activity_number)) %>% 
  group_by(day, activity_number) %>% 
  summarize(avg_value = mean(activity_counts)) %>% 
  ggplot(aes(x = activity_number, y = avg_value, color = day)) +
  geom_smooth(se = FALSE) +
  scale_x_discrete(limit = c(240,480,720,960,1200, 1440), labels = c("4","8","12","16","20","24")) +
  labs(title = "Average Daily Activity Time Course", x = "Time Course (hrs)", y = "Average Activity", color = "Day of the Week")

```

It appears that overall, activity builds during the day up until about 10am where it plateaus, usually dropping off around 8pm. However, we see a peak of evening activity on Friday's and a peak of midday activity around noon on Sunday's, with Sundays also having the lowest activity at the end of the day. 

**Fin**